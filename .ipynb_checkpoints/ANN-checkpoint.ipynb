{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040189,
     "end_time": "2021-02-03T10:33:46.901315",
     "exception": false,
     "start_time": "2021-02-03T10:33:46.861126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Biologically Inspired Computation (F20BC/F21BC), Coursework I "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03713,
     "end_time": "2021-02-03T10:33:47.052651",
     "exception": false,
     "start_time": "2021-02-03T10:33:47.015521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.006832Z",
     "start_time": "2022-10-27T22:23:47.632637Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:08.842927Z",
     "iopub.status.busy": "2022-10-22T16:14:08.842323Z",
     "iopub.status.idle": "2022-10-22T16:14:08.847810Z",
     "shell.execute_reply": "2022-10-22T16:14:08.846732Z",
     "shell.execute_reply.started": "2022-10-22T16:14:08.842825Z"
    },
    "papermill": {
     "duration": 0.050412,
     "end_time": "2021-02-03T10:33:47.142993",
     "exception": false,
     "start_time": "2021-02-03T10:33:47.092581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import time\n",
    "\n",
    "np.random.seed(3) # seed is set so that the random values give similar values at each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a Glimpse of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.040700Z",
     "start_time": "2022-10-27T22:23:51.011579Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:08.886819Z",
     "iopub.status.busy": "2022-10-22T16:14:08.886270Z",
     "iopub.status.idle": "2022-10-22T16:14:08.918583Z",
     "shell.execute_reply": "2022-10-22T16:14:08.917793Z",
     "shell.execute_reply.started": "2022-10-22T16:14:08.886784Z"
    },
    "papermill": {
     "duration": 0.149374,
     "end_time": "2021-02-03T10:33:47.331103",
     "exception": false,
     "start_time": "2021-02-03T10:33:47.181729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\") # reading the data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.068100Z",
     "start_time": "2022-10-27T22:23:51.046456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # to see the columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.148550Z",
     "start_time": "2022-10-27T22:23:51.074081Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:08.933557Z",
     "iopub.status.busy": "2022-10-22T16:14:08.933028Z",
     "iopub.status.idle": "2022-10-22T16:14:08.980781Z",
     "shell.execute_reply": "2022-10-22T16:14:08.979770Z",
     "shell.execute_reply.started": "2022-10-22T16:14:08.933524Z"
    },
    "papermill": {
     "duration": 0.105968,
     "end_time": "2021-02-03T10:33:47.477231",
     "exception": false,
     "start_time": "2021-02-03T10:33:47.371263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # to see the first five rows/instances/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.190834Z",
     "start_time": "2022-10-27T22:23:51.153464Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:08.982926Z",
     "iopub.status.busy": "2022-10-22T16:14:08.982588Z",
     "iopub.status.idle": "2022-10-22T16:14:09.006638Z",
     "shell.execute_reply": "2022-10-22T16:14:09.005599Z",
     "shell.execute_reply.started": "2022-10-22T16:14:08.982895Z"
    },
    "papermill": {
     "duration": 0.075553,
     "end_time": "2021-02-03T10:33:47.592945",
     "exception": false,
     "start_time": "2021-02-03T10:33:47.517392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # to see detailed information about the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.218999Z",
     "start_time": "2022-10-27T22:23:51.194333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum() # to check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.248632Z",
     "start_time": "2022-10-27T22:23:51.222955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() # to see the number of null values in each of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above output, we can infer that there are no duplicates and null values in the data (except for the Unnamed column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.274336Z",
     "start_time": "2022-10-27T22:23:51.250993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnosis'].value_counts() # to see the different values of the 'diagnosis' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.291607Z",
     "start_time": "2022-10-27T22:23:51.278370Z"
    }
   },
   "outputs": [],
   "source": [
    "# droping the unnamed and id features in the same dataframe\n",
    "df.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "X = df.drop('diagnosis', axis=1) # dropping the class/target attribute\n",
    "y = df.diagnosis # y set is the diagnosis column (class/target attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.322442Z",
     "start_time": "2022-10-27T22:23:51.294853Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:51:12.050798Z",
     "iopub.status.busy": "2022-10-22T16:51:12.050227Z",
     "iopub.status.idle": "2022-10-22T16:51:12.056449Z",
     "shell.execute_reply": "2022-10-22T16:51:12.055592Z",
     "shell.execute_reply.started": "2022-10-22T16:51:12.050763Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(y.map(lambda x: 1 if x=='M' else 0)) # encoding all Ms to 1s and Bs to 0s\n",
    "y = y.reshape(569,1) # to reshape y into a size compatible with the X set (number of rows=569, number of features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.337602Z",
     "start_time": "2022-10-27T22:23:51.326467Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting the X and y datasets into train and test sets for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.383019Z",
     "start_time": "2022-10-27T22:23:51.350516Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:51:19.492893Z",
     "iopub.status.busy": "2022-10-22T16:51:19.492175Z",
     "iopub.status.idle": "2022-10-22T16:51:19.498448Z",
     "shell.execute_reply": "2022-10-22T16:51:19.497769Z",
     "shell.execute_reply.started": "2022-10-22T16:51:19.492852Z"
    },
    "papermill": {
     "duration": 0.396672,
     "end_time": "2021-02-03T10:34:51.166347",
     "exception": false,
     "start_time": "2021-02-03T10:34:50.769675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalizing the X training and testing sets with min-max where the minimum value of a feature is made a 0 and the maximum value is made a 1, all other value become floats betwween 0 and 1\n",
    "X_train = (X_train - X_train.min())/(X_train.max() - X_train.min()) \n",
    "X_test = (X_test - X_test.min())/(X_test.max() - X_test.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of the final X and y training and testing datasets\n",
    "The output of .shape gives the dimensions in the following order- (number of rows, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.406530Z",
     "start_time": "2022-10-27T22:23:51.386289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.433572Z",
     "start_time": "2022-10-27T22:23:51.409488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.455229Z",
     "start_time": "2022-10-27T22:23:51.437530Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:51:20.254170Z",
     "iopub.status.busy": "2022-10-22T16:51:20.253543Z",
     "iopub.status.idle": "2022-10-22T16:51:20.261063Z",
     "shell.execute_reply": "2022-10-22T16:51:20.259741Z",
     "shell.execute_reply.started": "2022-10-22T16:51:20.254128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.481942Z",
     "start_time": "2022-10-27T22:23:51.462575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.504170Z",
     "start_time": "2022-10-27T22:23:51.486528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.522112Z",
     "start_time": "2022-10-27T22:23:51.507525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the values of the hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T22:23:51.549540Z",
     "start_time": "2022-10-27T22:23:51.532315Z"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = X.shape[1] # the number of features\n",
    "n_hidden_layers = 2 # default number of hidden layers\n",
    "n_hidden = [] # list to contain the number of nodes in each hidden layer\n",
    "activations = [] # list to contain the names of the activation functions selected for each layer\n",
    "lr = 0.1 # learning rate \n",
    "decay_rate = 0 # decay rate of the learning rate\n",
    "lr_decay = 'n' # if there should be a learning rate decay\n",
    "loss = 'c' # default loss type as cross entropy loss\n",
    "gradient_alg = 'b' # default gradient desecent algorithm as batch gradient descent\n",
    "n_outputs = 1 # number of output nodes\n",
    "dimensions = [] # to store the number of nodes in the input and all the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputting the values of the hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the number of input nodes/features (default =  30 )\n",
      "30\n",
      "Please enter the number of hidden layers\n",
      "3\n",
      "For layer 1:\n",
      "Please enter the number of hidden neurons in this layer (same as the size of this hidden layer) \n",
      "21\n",
      "For layer 2:\n",
      "Please enter the number of hidden neurons in this layer (same as the size of this hidden layer) \n"
     ]
    }
   ],
   "source": [
    "while True: # to loop until a valid number is given\n",
    "    print(\"Please enter the number of input nodes/features (default = \",X.shape[1],\")\")\n",
    "    n_inputs = int(input())\n",
    "    if n_inputs <= X_train.shape[1]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Try a smaller number less than or equal to \" + X_train.shape[1])\n",
    "\n",
    "# Taking the number of input features as requested\n",
    "X_train = X_train.iloc[:,:n_inputs]\n",
    "X_test = X_test.iloc[:,:n_inputs]\n",
    "\n",
    "print(\"Please enter the number of hidden layers\")\n",
    "n_hidden_layers = int(input())\n",
    "\n",
    "for i in range(n_hidden_layers):\n",
    "    print(\"For layer \" + str(i + 1) + \":\")\n",
    "    print(\n",
    "        \"Please enter the number of hidden neurons in this layer (same as the size of this hidden layer) \"\n",
    "    )\n",
    "    n_hidden.append(int(input()))\n",
    "\n",
    "for i in range(n_hidden_layers + 1):\n",
    "    print(\"For layer \" + str(i + 1) + \":\")\n",
    "    valid1 = False\n",
    "    while (valid1 == False): # to loop until a valid function is given\n",
    "        print(\n",
    "            \"Please choose an activation function (type one of- sigmoid/relu/tanh/softmax)\"\n",
    "        )\n",
    "        inp_a = input()\n",
    "        if (inp_a == \"sigmoid\" or inp_a == \"relu\" or inp_a == \"tanh\"):\n",
    "            valid1 = True\n",
    "            activations.append(inp_a)\n",
    "            \n",
    "        elif (inp_a == \"softmax\"):\n",
    "            valid1 = True\n",
    "            activations.append(inp_a)\n",
    "            n_outputs = 2\n",
    "\n",
    "        else:\n",
    "            print(\"Try Again\")\n",
    "\n",
    "print(\"Please enter the number of epochs(1-20000)\")\n",
    "epochs = int(input())\n",
    "\n",
    "while True: # to loop until a valid number is given\n",
    "    print(\"Please enter a learning rate <= 1\")\n",
    "    lr = float(input())\n",
    "    if lr <= 1:\n",
    "        break\n",
    "\n",
    "print(\"Should the learning rate decay with each layer? (type y/n)\")\n",
    "lr_decay = input()\n",
    "\n",
    "valid2 = False\n",
    "while (valid2 == False): # to loop until a valid function is given\n",
    "    print(\n",
    "        \"Please choose a loss function (cross entropy loss (type c)/ hinge loss (type h)/ squared hinge loss(type s))\"\n",
    "    )\n",
    "    loss = input()\n",
    "\n",
    "    if (loss == \"c\" or loss == \"h\" or loss == \"s\"):\n",
    "        valid2 = True\n",
    "\n",
    "    else:\n",
    "        print(\"Try Again\")\n",
    "\n",
    "valid3 = False\n",
    "while (valid3 == False): # to loop until a valid algorithm is given\n",
    "    print(\n",
    "        \"Please choose a gradient descent algorithm (batch gradient descent (type b)/ stochastic gradient descent (type s)/ and mini-batch gradient descent (type m))\"\n",
    "    )\n",
    "    gradient_alg = input()\n",
    "\n",
    "    # for mini batch gradient descent\n",
    "    if (gradient_alg == \"m\"):\n",
    "        valid3 = True\n",
    "        print(\"Please enter the number of batches\")\n",
    "        batch_size = int(input())\n",
    "    \n",
    "    # for batch gradient descent\n",
    "    elif (gradient_alg == \"b\"): # batch size of a mini batch would be the same as the number of data samples/rows\n",
    "        valid3 = True\n",
    "        batch_size = X.shape[0] #X_train\n",
    "\n",
    "    # for stochatic gradient descent\n",
    "    elif (gradient_alg == \"s\"): # batch size of a mini batch would be the 1 as costs are cmomputed for each of the samples\n",
    "        valid3 = True\n",
    "        batch_size = 1\n",
    "        \n",
    "    else:\n",
    "        print(\"Try Again\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.644Z"
    }
   },
   "outputs": [],
   "source": [
    "# to append the number of input festures into the dimensions list\n",
    "dimensions.append(n_inputs)\n",
    "# to add the number of nodes in each of the hidden layers from n_hidden into the dimensions list\n",
    "dimensions.extend(n_hidden)\n",
    "# to append the number of output nodes into the dimensions list\n",
    "dimensions.append(n_outputs)\n",
    "\n",
    "# to convert all elements in dimensions to numpy ints\n",
    "dimensions = np.array(dimensions, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.378798,
     "end_time": "2021-02-03T10:34:52.722742",
     "exception": false,
     "start_time": "2021-02-03T10:34:52.343944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Intializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.648Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:09.971180Z",
     "iopub.status.busy": "2022-10-22T16:14:09.970860Z",
     "iopub.status.idle": "2022-10-22T16:14:09.979297Z",
     "shell.execute_reply": "2022-10-22T16:14:09.978363Z",
     "shell.execute_reply.started": "2022-10-22T16:14:09.971150Z"
    },
    "papermill": {
     "duration": 0.399757,
     "end_time": "2021-02-03T10:34:54.320666",
     "exception": false,
     "start_time": "2021-02-03T10:34:53.920909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "    \n",
    "    \"\"\"\n",
    "    To initialize all the weights and biases\n",
    "    \n",
    "        Parameters:\n",
    "            dimensions - list with the input features and number of nodes in all the layers of the network\n",
    "\n",
    "        Returns:\n",
    "            parameters - dictionary with the values of the weights and biases where 'W' is the prefix for weights and 'b' for the bias\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    for i in range (len(dimensions)-1):\n",
    "        parameters[\"W\"+str(i+1)] = np.random.randn(dimensions[i+1],dimensions[i]) / np.sqrt(dimensions[i]) # giving random values with the specified dimensions\n",
    "        parameters[\"b\"+str(i+1)] = np.zeros((dimensions[i+1],1)) # setting the bias to zero vector with the specified dimensions\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.37996,
     "end_time": "2021-02-03T10:34:55.084563",
     "exception": false,
     "start_time": "2021-02-03T10:34:54.704603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    To compute the value of the activation or the output of the layer with sigmoid activation function.\n",
    "    \n",
    "        Parameters:\n",
    "            Z - linear activation value- \n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            A - node output\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "# ReLU (rectified linear unit)\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    To compute the value of the activation or the output of the layer with ReLU activation function.\n",
    "    \n",
    "        Parameters:\n",
    "            Z - linear activation value- \n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            A - node output\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "# Hyperbolic tangent\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    To compute the value of the activation or the output of the layer with the tanh function.\n",
    "    \n",
    "        Parameters:\n",
    "            Z - linear activation value- \n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            A - node output\n",
    "    \"\"\"\n",
    "    A = np.tanh(Z)\n",
    "    return A\n",
    "\n",
    "# Softmax function\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    To compute the value of the activation or the output of the layer with softmax funxtion.\n",
    "    \n",
    "        Parameters:\n",
    "            Z - linear activation value- \n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            A - node output\n",
    "    \"\"\"\n",
    "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.657Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:09.982369Z",
     "iopub.status.busy": "2022-10-22T16:14:09.982058Z",
     "iopub.status.idle": "2022-10-22T16:14:09.990806Z",
     "shell.execute_reply": "2022-10-22T16:14:09.989768Z",
     "shell.execute_reply.started": "2022-10-22T16:14:09.982339Z"
    },
    "papermill": {
     "duration": 0.394039,
     "end_time": "2021-02-03T10:34:56.62267",
     "exception": false,
     "start_time": "2021-02-03T10:34:56.228631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def z_comput(W, b, prev_A):\n",
    "    \n",
    "    \"\"\"\n",
    "    To compute the value of the linear activation\n",
    "    \n",
    "        Parameters:\n",
    "            W - weight matrix of the layer\n",
    "            b - bias matrix of the layer\n",
    "            prev_A - previous activation values from the previous layer\n",
    "\n",
    "        Returns:\n",
    "            Z - linear activation value\n",
    "            cache - tuple with the previous activations, weight matrix, and bias matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,prev_A)+b # calculating the value of the linear activation\n",
    "    lcache = (prev_A, W, b) # storing the values involved in this calculation in a tuple- linear cache\n",
    "\n",
    "    return Z, lcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.661Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:09.992948Z",
     "iopub.status.busy": "2022-10-22T16:14:09.992504Z",
     "iopub.status.idle": "2022-10-22T16:14:10.002176Z",
     "shell.execute_reply": "2022-10-22T16:14:10.001005Z",
     "shell.execute_reply.started": "2022-10-22T16:14:09.992906Z"
    },
    "papermill": {
     "duration": 0.396919,
     "end_time": "2021-02-03T10:34:57.400844",
     "exception": false,
     "start_time": "2021-02-03T10:34:57.003925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_prop(W, b, prev_A, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    To compute the value of the activation/ layer output\n",
    "    \n",
    "        Parameters:\n",
    "            W - weight matrix of the layer\n",
    "            b - bias matrix of the layer\n",
    "            prev_A - previous activation values from the previous layer\n",
    "            activation - activation function for the given layer\n",
    "\n",
    "        Returns:\n",
    "            A - node output\n",
    "            cache - tuple with the previous activations, weight matrix, bias matrix, and linear activation (Z)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.zeros((W.shape[0], prev_A.shape[1])) # initializing A\n",
    "    cache = ()\n",
    "\n",
    "    # computing the value of the output based on the activation function     \n",
    "    if activation == \"relu\":\n",
    "        Z, lcache = z_comput(W, b, prev_A) # obtaining the value of Z and weight, bias, previous activations\n",
    "        A = relu(Z)\n",
    "        zcache = Z\n",
    "        cache = (lcache, zcache) # combined caches with previous activations, weights, bias, Z- linear activation value\n",
    "\n",
    "    elif activation == 'sigmoid':\n",
    "        Z, lcache = z_comput(W, b, prev_A)\n",
    "        A = sigmoid(Z)\n",
    "        zcache = Z\n",
    "        cache = (lcache, zcache)\n",
    "\n",
    "    elif activation == 'tanh':\n",
    "        Z, lcache = z_comput(W, b, prev_A)\n",
    "        A = tanh(Z)\n",
    "        zcache = Z\n",
    "        cache = (lcache, zcache)\n",
    "\n",
    "    elif activation == 'softmax':\n",
    "        Z, lcache = z_comput(W, b, prev_A)\n",
    "        A = softmax(Z)\n",
    "        zcache = Z\n",
    "        cache = (lcache, zcache) \n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.666Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.004517Z",
     "iopub.status.busy": "2022-10-22T16:14:10.003914Z",
     "iopub.status.idle": "2022-10-22T16:14:10.014594Z",
     "shell.execute_reply": "2022-10-22T16:14:10.013591Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.004474Z"
    },
    "papermill": {
     "duration": 0.396222,
     "end_time": "2021-02-03T10:34:58.180613",
     "exception": false,
     "start_time": "2021-02-03T10:34:57.784391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_prop_layers(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    To compute the value of the activation for all the layers\n",
    "    \n",
    "        Parameters:\n",
    "            X - X dataset without class attribute\n",
    "            parameters - weight matrix and bias matrix of all the layers\n",
    "            activations - names of the activation functions of all the layers\n",
    "\n",
    "        Returns:\n",
    "            A - activation value of all layers- of the forward pass\n",
    "            cache - tuple with the previous activations, weight matrices, bias matrices and linear activations (Z)\n",
    "    \"\"\"\n",
    "    A = X # the previous activation matrix is initialized with the X matrix for the calculation of the next activation value\n",
    "    caches = []\n",
    "    l = len(parameters)//2 # number of layers is half the size of the parameters dictionary (= number of weight/bias matrices)\n",
    "    for i in range (l): # computing the activations for all layers and compiling all the caches\n",
    "        prev_A = A # represents the previous activation value computed\n",
    "        A, cache = forward_prop(parameters[\"W\"+str(i+1)],parameters[\"b\"+str(i+1)], prev_A, activations[i])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the cost/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.670Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.016261Z",
     "iopub.status.busy": "2022-10-22T16:14:10.015789Z",
     "iopub.status.idle": "2022-10-22T16:14:10.031771Z",
     "shell.execute_reply": "2022-10-22T16:14:10.030582Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.016208Z"
    },
    "papermill": {
     "duration": 0.397293,
     "end_time": "2021-02-03T10:34:58.956961",
     "exception": false,
     "start_time": "2021-02-03T10:34:58.559668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost(A, Y, loss):\n",
    "    \"\"\"\n",
    "    To compute the cost/loss\n",
    "    \n",
    "        Parameters:\n",
    "            Y - actual values of the target class attribute\n",
    "            A - activation values or predicted values\n",
    "            loss- loss function to use\n",
    "\n",
    "        Returns:\n",
    "            cost1 - cost calculated using the specified loss function\n",
    "    \"\"\"\n",
    "    n = Y.shape[1] # number of samples\n",
    "    if loss == \"c\": # for cross entropy loss\n",
    "        cost = (1./n) * (-np.dot(Y,np.log(A+1e-15).T) - np.dot(1-Y, np.log(1-A+1e-15).T))\n",
    "    elif loss == \"h\": # for hinge loss\n",
    "        cost = np.mean(np.maximum(0, 1 - Y * A))\n",
    "    elif loss == \"s\": # for squared hinge loss\n",
    "        cost = np.mean(np.maximum(0, 1 - Y * A))**2\n",
    "\n",
    "    cost1 = np.squeeze(cost) # removing one-dimensional elements or extracting the cost value from the numpy array\n",
    "    return cost1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.382751,
     "end_time": "2021-02-03T10:34:59.720621",
     "exception": false,
     "start_time": "2021-02-03T10:34:59.33787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.674Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.033931Z",
     "iopub.status.busy": "2022-10-22T16:14:10.033397Z",
     "iopub.status.idle": "2022-10-22T16:14:10.043794Z",
     "shell.execute_reply": "2022-10-22T16:14:10.042568Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.033869Z"
    },
    "papermill": {
     "duration": 0.396545,
     "end_time": "2021-02-03T10:35:00.498503",
     "exception": false,
     "start_time": "2021-02-03T10:35:00.101958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def derivative_compute(dZ, lcache):\n",
    "    \"\"\"\n",
    "    To compute the derivatives/gradients of the weights, biases, and activation value\n",
    "    \n",
    "        Parameters:\n",
    "            dZ - gradient of cost with respect to Z (linear activation)\n",
    "            lcache - linear cache with the values of weights, biases and activations\n",
    "\n",
    "        Returns:\n",
    "            dprev_A - previous activation value\n",
    "            dW - derivative of the weights of that layer\n",
    "            db - derivate of the weights of that layer\n",
    "    \"\"\"\n",
    "    \n",
    "    prev_A, W, b = lcache # extracting each of the values in the lcache\n",
    "    n = prev_A.shape[1] # number of samples\n",
    "    \n",
    "    # averaging the values over the nodes of the layers\n",
    "    dW = 1./n * np.dot(dZ, prev_A.T)\n",
    "    db = 1./n * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dprev_A, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.678Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.046278Z",
     "iopub.status.busy": "2022-10-22T16:14:10.045609Z",
     "iopub.status.idle": "2022-10-22T16:14:10.055321Z",
     "shell.execute_reply": "2022-10-22T16:14:10.053975Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.046227Z"
    },
    "papermill": {
     "duration": 0.37578,
     "end_time": "2021-02-03T10:35:01.255398",
     "exception": false,
     "start_time": "2021-02-03T10:35:00.879618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward_prop(dA, activation, cache):\n",
    "    \"\"\"\n",
    "    To compute the value of derivatives for a layer\n",
    "    \n",
    "        Parameters:\n",
    "            dA - derivative of the activation value\n",
    "            activation- activation function to use\n",
    "            cache - weights, biases, alinear activations and layer outputs\n",
    "\n",
    "        Returns:\n",
    "            dprev_A - previous activation value\n",
    "            dW - derivative of the weights of that layer\n",
    "            db - derivate of the weights of that layer\n",
    "    \"\"\"\n",
    "    \n",
    "    lcache, zcache = cache\n",
    "    Z = zcache # the value of Z from zcache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = np.array(dA, copy=True) # dz is made into a numpy array for computations\n",
    "        dZ[Z <= 0] = 0 # dz is set to 0 if Z is greater than or equal to 0 \n",
    "        dprev_A, dW, db = derivative_compute(dZ, lcache) # to get the derivatives using the derivative_compute function\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        dprev_A, dW, db = derivative_compute(dZ, lcache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = np.multiply(dA,\n",
    "                         (1- ((np.tanh(Z))*(np.tanh(Z)))))\n",
    "        dprev_A, dW, db = derivative_compute(dZ, lcache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = np.multiply(dA, np.exp(Z) / sum(np.exp(Z)) * (1. - np.exp(Z) / sum(np.exp(Z))))\n",
    "        dprev_A, dW, db = derivative_compute(dZ, lcache)\n",
    "\n",
    "    return dprev_A, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.687Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.057147Z",
     "iopub.status.busy": "2022-10-22T16:14:10.056823Z",
     "iopub.status.idle": "2022-10-22T16:14:10.067963Z",
     "shell.execute_reply": "2022-10-22T16:14:10.066489Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.057115Z"
    },
    "papermill": {
     "duration": 0.396775,
     "end_time": "2021-02-03T10:35:02.045117",
     "exception": false,
     "start_time": "2021-02-03T10:35:01.648342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward_prop_layers(last_A, Y, caches, activations):\n",
    "    \"\"\"\n",
    "    To compute the value of derivatives for a layer\n",
    "    \n",
    "        Parameters:\n",
    "            dA - gradient of the activation value\n",
    "            activation- activation function to use\n",
    "            cache - weights, biases, alinear activations and layer outputs\n",
    "\n",
    "        Returns:\n",
    "            cost1 - cost calculated using the specified loss function\n",
    "    \"\"\"\n",
    "\n",
    "    gradient_dict = {} # dictionary to store the gradients\n",
    "    l = len(caches) # number of layers = length of the caches\n",
    "    \n",
    "    Y = Y.reshape(last_A.shape) # reshping or chanding the dimensions of Y for compatibility for computations\n",
    "    \n",
    "    current_cache = caches[l - 1] # the last cache is chosen first to proceed backwards from the alast layer\n",
    "    \n",
    "    # calculating the first activation gradient\n",
    "    gradient_dict[\"dA\" + str(l)] = -(np.divide(Y, last_A+1e-15) -\n",
    "                                     np.divide(1 - Y, 1 - last_A+1e-15))\n",
    "    \n",
    "    # the first set of gradients for Aactivations, weights and biases are calculated with the computed derivative of A\n",
    "    gradient_dict[\"dA\" + str(l - 1)], gradient_dict[\n",
    "        \"dW\" + str(l)], gradient_dict[\"db\" + str(l)] = backward_prop(\n",
    "            gradient_dict[\"dA\" + str(l)], activations[l - 1], current_cache)\n",
    "\n",
    "    # calculating the gradients of weights, biases and activation values for all layers, starting from the second last layer in this loop\n",
    "    for i in reversed(range(l - 1)):\n",
    "        current_cache = caches[i]\n",
    "        dprev_A, dW, db = backward_prop(gradient_dict[\"dA\" + str(i + 1)],\n",
    "                                        activations[i], current_cache)\n",
    "        gradient_dict[\"dA\" + str(i)] = dprev_A\n",
    "        gradient_dict[\"dW\" + str(i + 1)] = dW\n",
    "        gradient_dict[\"db\" + str(i + 1)] = db\n",
    "\n",
    "    return gradient_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.693Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:43:56.077019Z",
     "iopub.status.busy": "2022-10-22T16:43:56.076414Z",
     "iopub.status.idle": "2022-10-22T16:43:56.086364Z",
     "shell.execute_reply": "2022-10-22T16:43:56.085105Z",
     "shell.execute_reply.started": "2022-10-22T16:43:56.076966Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grad_dict, lr):\n",
    "    \"\"\"\n",
    "    To update the parameters (weights and biases)\n",
    "    \n",
    "        Parameters:\n",
    "            parameters - dictionary with weights and biases to update\n",
    "            grad_dict - dictionary with gradients of weights, biases and activations\n",
    "            lr- learning rate\n",
    "\n",
    "        Returns:\n",
    "            parameters - updated parameters dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    l = len(parameters) // 2 # number of layers in the model/network\n",
    "    \n",
    "    # for al layers, parameter = original parameter value - learning rate * gradient of the parameter calculation is done\n",
    "    for i in range(l): \n",
    "        parameters[\"W\" + str(i+1)] = parameters[\"W\" + str(i+1)] - lr * grad_dict[\"dW\"+str(i+1)]        \n",
    "        parameters[\"b\" + str(i+1)] = parameters[\"b\" + str(i+1)] - lr * grad_dict[\"db\"+str(i+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.38464,
     "end_time": "2021-02-03T10:35:02.836689",
     "exception": false,
     "start_time": "2021-02-03T10:35:02.452049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating Mini Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.697Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:10.069643Z",
     "iopub.status.busy": "2022-10-22T16:14:10.069171Z",
     "iopub.status.idle": "2022-10-22T16:14:10.078989Z",
     "shell.execute_reply": "2022-10-22T16:14:10.077911Z",
     "shell.execute_reply.started": "2022-10-22T16:14:10.069599Z"
    },
    "papermill": {
     "duration": 0.401724,
     "end_time": "2021-02-03T10:35:03.620172",
     "exception": false,
     "start_time": "2021-02-03T10:35:03.218448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    To create mini batches randomly\n",
    "    \n",
    "        Parameters:\n",
    "        X - input data without the target attribute\n",
    "        y - target attribute with actual values\n",
    "        batch_size - size of the mini-batches\n",
    "\n",
    "        Returns:\n",
    "        mini_batches - batches of the X and y sets\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    n = X.shape[1] # number of data samples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # shuffling the data to avoid having the same data together each time\n",
    "    shuffle = list(np.random.permutation(n))\n",
    "    X_shuffle = X.iloc[:, shuffle]\n",
    "    y_shuffle = y[:, shuffle].reshape((1,n))\n",
    "\n",
    "    num_complete_minibatches = n//batch_size # number of batches to make from the batch size\n",
    "    \n",
    "    # loop for making the batches as many as the number of batches calculated\n",
    "    for i in range(num_complete_minibatches):\n",
    "        \n",
    "        mini_batch_X = X_shuffle.iloc[:,i*batch_size:(i+1)*batch_size]\n",
    "        mini_batch_y = y_shuffle[:,i*batch_size:(i+1)*batch_size]\n",
    "     \n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # when the number of data samples is lesser than the batch size considered, the last batch will have a size lesser than the batch size considered\n",
    "    if n % batch_size != 0:\n",
    "        \n",
    "        mini_batch_X = X_shuffle.iloc[:,num_complete_minibatches*batch_size:n]\n",
    "        mini_batch_y = y_shuffle[:,num_complete_minibatches*batch_size:n]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.389355,
     "end_time": "2021-02-03T10:35:06.730605",
     "exception": false,
     "start_time": "2021-02-03T10:35:06.34125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.702Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:47:33.871394Z",
     "iopub.status.busy": "2022-10-22T16:47:33.871010Z",
     "iopub.status.idle": "2022-10-22T16:47:33.881898Z",
     "shell.execute_reply": "2022-10-22T16:47:33.880417Z",
     "shell.execute_reply.started": "2022-10-22T16:47:33.871360Z"
    },
    "papermill": {
     "duration": 0.407701,
     "end_time": "2021-02-03T10:35:07.524595",
     "exception": false,
     "start_time": "2021-02-03T10:35:07.116894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ann(X, y, dimensions, lr, lr_decay, batch_size, epochs, loss, activations, gradient_alg):\n",
    "    \"\"\"\n",
    "    Represents the artificial neural network with all components compiled together\n",
    "    \n",
    "        Parameters:\n",
    "            X - input data without the class attribute\n",
    "            y - actual values of the class attribute\n",
    "            lr - learning rate\n",
    "            lr_decay - whether to have a learning rate decay/schedule or not\n",
    "            batch_size - size of the batches of data to consider\n",
    "            epochs - number of iterations the computations are to be done for\n",
    "            loss - loss function to use for the error calculation\n",
    "            activations - list of the activation functions to use for each layer\n",
    "            gradient_alg - gradient descent algorithm to use\n",
    "\n",
    "        Returns:\n",
    "            parameters1 - list of parameters, dimensions, activations, costs, lr, and batch_size\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    n = X.shape[1] # number of samples\n",
    "    costs = [] # list of costs in each epoch\n",
    "    \n",
    "    # for learning rate decay\n",
    "    alpha_zero = 0.2 \n",
    "    bool_decay = False # there is no decay enabled by default \n",
    "    if(lr_decay == 'y'):\n",
    "        bool_decay = True\n",
    "        lr_decay_rate =  0.01 # decay rate\n",
    "    \n",
    "    parameters = initialize_parameters(dimensions) # initial values for the weights and biases\n",
    "    \n",
    "    if (gradient_alg == \"b\"): # the batch size is set as the number of samples, for batch gradient descent\n",
    "        batch_size = X.shape[1]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # forming mini batches with the given batch size\n",
    "        minibatches = random_mini_batches(X, y, batch_size)\n",
    "        cost_total = 0\n",
    "        \n",
    "        # iterating over all the minibatches\n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            (minibatch_X,minibatch_Y) = minibatch # to get the minibatch\n",
    "            \n",
    "            # forward propgation/pass is done and it outputs the activations and caches\n",
    "            last_A, caches = forward_prop_layers(minibatch_X, parameters, activations) \n",
    "            \n",
    "            # error calculation using the loss function\n",
    "            cost_total += compute_cost(last_A, minibatch_Y, loss) \n",
    "            \n",
    "            # backward propogation/pass outputs the gradients\n",
    "            gradients = backward_prop_layers(last_A, minibatch_Y, caches, activations)\n",
    "            \n",
    "            # weights and biases are updated using the learning rate and gradients from backward pass\n",
    "            parameters = update_parameters(parameters, gradients, lr)\n",
    "            \n",
    "            #If user has configured 'y' for learning decay, a learning rate schedule is implemeted\n",
    "            if(bool_decay):\n",
    "                lr = math.pow(0.5, math.floor((1+i)/10.0))  #Step decay \n",
    "                  \n",
    "            else:\n",
    "                bool_decay = False\n",
    "                lr = lr\n",
    "\n",
    "        # updating the cost by taking its total mean from the costs calculated in the different batches wherever applicable\n",
    "        if gradient_alg == 'b':\n",
    "            cost_avg = cost_total\n",
    "        else: \n",
    "            cost_avg = cost_total / n \n",
    "            \n",
    "        # printing the cost for every 10 epochs\n",
    "        if i %10 == 0:\n",
    "            print (\"Cost/error after epoch %i: %f\" %(i, cost_avg))\n",
    "        costs.append(cost_avg)\n",
    "          \n",
    "    end = time.time()\n",
    "    total_time = end-start # to calculate the total time taken for training the model\n",
    "    \n",
    "    print (\"Cost/error after all epochs: \",(cost_avg))\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('Computed cost')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.title(\"Learning rate = \" + str(lr))\n",
    "    plt.show()\n",
    "\n",
    "    # all values to return for later use in visualizations and predictions\n",
    "    parameters1 = [parameters, dimensions, activations, costs, lr, batch_size, total_time]\n",
    "    \n",
    "    return parameters1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.364643,
     "end_time": "2021-02-03T10:35:08.237941",
     "exception": false,
     "start_time": "2021-02-03T10:35:07.873298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.706Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2022-10-22T16:47:34.666475Z",
     "iopub.status.busy": "2022-10-22T16:47:34.666115Z",
     "iopub.status.idle": "2022-10-22T16:47:34.672594Z",
     "shell.execute_reply": "2022-10-22T16:47:34.671309Z",
     "shell.execute_reply.started": "2022-10-22T16:47:34.666444Z"
    },
    "papermill": {
     "duration": 0.363897,
     "end_time": "2021-02-03T10:35:08.985323",
     "exception": false,
     "start_time": "2021-02-03T10:35:08.621426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activations):\n",
    "    \"\"\"\n",
    "    To predict the labels after running the forward pass\n",
    "    \n",
    "        Arguments:\n",
    "        X - input data without the class attribute\n",
    "        y - actual values of the class attribute\n",
    "        parameters - parameters (weights and biases) of the model after training\n",
    "        activations - list of the activation functions to use for each layer\n",
    "\n",
    "        Returns:\n",
    "        pred -- predictions for the given input data- X\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[1] # number of samples\n",
    "    pred = np.zeros((1,n)) # initializing the predictions\n",
    "    \n",
    "    # Forward propagation with the given values\n",
    "    A, caches = forward_prop_layers(X,parameters,activations)\n",
    "    \n",
    "    # for every prediction greater than 0.39, make it a 1 else a 0\n",
    "    for i in range(0, A.shape[1]):\n",
    "        if A[0,i] >= 0.39:\n",
    "            pred[0,i] = 1\n",
    "        else:\n",
    "            pred[0,i] = 0\n",
    "    \n",
    "    # accuracy is computed by dividing the sum of the number of correct predictions by the total number of samples\n",
    "    accuracy = np.sum((pred == y)/n)\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(accuracy))\n",
    "        \n",
    "    return pred, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.390011,
     "end_time": "2021-02-03T10:35:09.762944",
     "exception": false,
     "start_time": "2021-02-03T10:35:09.372933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training and Testing with inputted Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.713Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = ann(X_train.T, y_train.T, dimensions, lr, lr_decay, batch_size, epochs, loss, activations, gradient_alg)\n",
    "params = parameters[0]\n",
    "activations1 = parameters[2]\n",
    "\n",
    "# Accuracy is displayed in the range of 0-1 where 1 is 100%.\n",
    "print('Training Accuracy')\n",
    "pred_train, acc = predict(X_train.T, y_train.T, params, activations1)\n",
    "print('Testing Accuracy')\n",
    "pred_test, acc = predict(X_test.T, y_test.T, params, activations1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.4028,
     "end_time": "2021-02-03T10:40:11.392156",
     "exception": false,
     "start_time": "2021-02-03T10:40:10.989356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.717Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-22T16:14:14.901699Z",
     "iopub.status.busy": "2022-10-22T16:14:14.901394Z",
     "iopub.status.idle": "2022-10-22T16:14:15.416834Z",
     "shell.execute_reply": "2022-10-22T16:14:15.415785Z",
     "shell.execute_reply.started": "2022-10-22T16:14:14.901650Z"
    },
    "papermill": {
     "duration": 0.430455,
     "end_time": "2021-02-03T10:40:12.217079",
     "exception": false,
     "start_time": "2021-02-03T10:40:11.786624",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some of the tested combinations:- traning function call has been followed tby the classification accuracy percentage\n",
    "\n",
    "# parameters = ann(X_train.T, y_train.T, [30,8,8,8,1], 0.01, \"n\", 426, 800, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #90.9%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,8,8,8,1], 0.01, \"n\", 426, 800, \"h\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #88.9%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,21,25,59,1], 0.01, \"n\", 426, 1000, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #92.3%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,20,25,21,59,1], 0.01, \"n\", 426, 1500, \"c\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #94.4%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,20,20,20,20,1], 0.00095, \"y\", 16, 1000, \"c\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"m\")  #94%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,20,21,20,25,1], 0.00095, \"n\", 16, 1500, \"c\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"m\") #92%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,20,20,20,1], 0.00095, \"y\", 16, 1500, \"c\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"m\")  #94.4%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,25,25,21,20,1], 0.01, \"n\", 1, 150, \"c\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"s\") #87.4%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,10,20,25,1], 0.01, \"n\", 1, 200, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"s\") # 88%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,10,20,25,1], 0.01, \"n\", 426, 4500, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #93%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,200,200,200,200,1], 0.00095, \"y\", 16, 800, \"h\", [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"m\") #93%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,21,25,30,1], 0.01, \"n\", 426, 1000, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") #90%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,21,25,30,1], 0.01, \"y\", 426, 500, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"s\") #92.3%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,10,20,25,1], 0.01, \"n\", 426, 4500, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\") # 92%\n",
    "# parameters = ann(X_train.T, y_train.T, [30,20,21,25,1], 0.01, \"n\", 426, 150, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"s\")# 90.2%\n",
    "\n",
    "parameters = ann(X_train.T, y_train.T, [30,10,8,8,1], 0.03, \"n\", 426, 1000, \"c\", [\"relu\",\"tanh\",\"relu\",\"sigmoid\"], \"b\")\n",
    "params = parameters[0]\n",
    "activations1 = parameters[2]\n",
    "pred, acc = predict(X_test.T, y_test.T, params, activations1)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Confusion matrix where tn-true negative, fp- false positive, fn- false negative, tp- true positive\n",
    "tn, fp, fn, tp = confusion_matrix(y_test.reshape(1,y_test.shape[0])[0],pred[0]).ravel()\n",
    "print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cost values charted after increasing epochs and decreasing learning rates simultaneously\n",
    "lrs = [1e-1,1e-2,2e-2,3e-2,5e-2,6e-2,7e-2,8e-2,9e-2,1e-3,2e-3,3e-3,5e-3,6e-3,7e-3,8e-3,9e-3]\n",
    "epoch = [50,60,80,100,140,200,250,300,500,1000,1500,2000]\n",
    "\n",
    "for epochs in epoch:\n",
    "    for lr1 in lrs:\n",
    "        parameters = ann(X_train.T, y_train.T, [30,10,8,8,1], lr1, \"n\", 426, epochs, \"c\", [\"relu\",\"tanh\",\"relu\",\"sigmoid\"], \"b\")\n",
    "        print(\"Accuracy\")\n",
    "        predictions, acc = predict(X_test.T, y_test.T, parameters[0], parameters[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reporting Accuracy v/s Learning Rates, and Training time v/s Learning Rates using the follwing hyperparameters\n",
    "lrs = [1e-1,1e-2,2e-2,3e-2,5e-2,6e-2,7e-2,8e-2,9e-2,1e-3,2e-3,3e-3,5e-3,6e-3,7e-3,8e-3,9e-3]\n",
    "accs = []\n",
    "times  = []\n",
    "for lr1 in lrs:\n",
    "    parameters = ann(X_train.T, y_train.T, [30,21,25,30,1], lr1, \"y\", 426, 500, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\")\n",
    "    print(\"Accuracy\")\n",
    "    predictions, acc = predict(X_test.T, y_test.T, parameters[0], parameters[2])\n",
    "    accs.append(acc)\n",
    "    times.append(parameters[6])\n",
    "        \n",
    "plt.scatter(lrs, accs)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('lr')\n",
    "plt.title(\"Accuracy v/s Learning Rates\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(lrs, times)\n",
    "plt.ylabel('Traning Time in seconds')\n",
    "plt.xlabel('lr')\n",
    "plt.title(\"Training time v/s Learning Rates\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-27T22:23:47.733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reporting Accuracy v/s Number of Epochs, and Training time v/s Number of Epochs using the follwing hyperparameters\n",
    "epoch = [50,60,80,100,140,200,250,300,500,1000,1500,2000]\n",
    "accs = []\n",
    "times  = []\n",
    "for epochs in epoch:\n",
    "    parameters = ann(X_train.T, y_train.T, [30,21,25,30,1], lr1, \"y\", 426, 500, \"c\", [\"relu\",\"relu\",\"relu\",\"sigmoid\"], \"b\")\n",
    "    print(\"Accuracy\")\n",
    "    predictions = predict(X_test.T, y_test.T, parameters[0], parameters[2])\n",
    "    accs.append(acc)\n",
    "    times.append(parameters[6])\n",
    "        \n",
    "plt.scatter(epoch, accs)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.title(\"Accuracy v/s Number of Epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(epoch, times)\n",
    "plt.ylabel('Training time in seconds')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.title(\"Training time v/s Number of Epochs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
